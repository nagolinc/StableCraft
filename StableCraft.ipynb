{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyM/zbf6HI0UncRb303iSZ5o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nagolinc/StableCraft/blob/main/StableCraft.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1OcpQv1bMUAg"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/openai/whisper.git \n",
        "!pip install transformers\n",
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flask-ngrok2"
      ],
      "metadata": {
        "id": "GNSq52_cOHp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install diffusers"
      ],
      "metadata": {
        "id": "KB1ukrPsOhr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/nagolinc/StableCraft.git"
      ],
      "metadata": {
        "id": "xpzXa46INRWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd StableCraft"
      ],
      "metadata": {
        "id": "3dOGmY7vNqe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"HF_TOKEN\"]=<your token here>#huggingface token\n",
        "os.environ[\"NGROK_TOKEN\"]=<your token here>#ngrok token"
      ],
      "metadata": {
        "id": "naJpnSmfNCez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iAlvCNGXQbaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!python flask_app.py --diffusion_model hakurei/waifu-diffusion\n",
        "!python flask_app.py --diffusion_model doohickey/doodad  --no_fp16"
      ],
      "metadata": {
        "id": "kS6TUFDeODVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "TestPromptGeneration=False\n",
        "\n",
        "if TestPromptGeneration:\n",
        "  tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\",torch_dtype=torch.float16)\n",
        "  text_model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\",torch_dtype=torch.float16)\n",
        "  generator = pipeline(task=\"text-generation\", model=text_model, tokenizer=tokenizer,device=0)\n"
      ],
      "metadata": {
        "id": "4mMv_aaZOWbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompts=['abstract sculpture of a cat by Jeff Koons','ancient roman sculpture of a cat','a butter sculpture of a cat, a cat made out of butter','a bronze statue of a cat']"
      ],
      "metadata": {
        "id": "YB7CqOHROmho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if TestPromptGeneration:\n",
        "  textInput=\"\\ndescription:\\n\".join([s.strip() for s in prompts])\n",
        "  output=generator(textInput,max_new_tokens=200,return_full_text=False)[0]['generated_text']\n",
        "  print(output)"
      ],
      "metadata": {
        "id": "yvVzcDzDOyUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save your project to a zip file (which can then be shared anywhere, no colab needed!)\n",
        "!zip -r sampleProject.zip /content/StableCraft/static"
      ],
      "metadata": {
        "id": "fdtQAT4IdHr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K68qChndHkAb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}